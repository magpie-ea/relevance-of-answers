---
title: "Qualitative analysis"
author: "Omar"
format: 
  html:
    code-fold: false
    self-contained: true
execute:
  error: true
  warning: true
  message: true
editor:
  markdown:
    wrap: sentence
---

## Preamble

I drop the `group` column (`relevant` vs `helpful`) since we found no difference.


Everything else exactly follows Michael's R code from the round 2 analysis:

* I set non-answers to positive polarity.
* A response is `deviant` if there is no `belief_change` and the `AnswerCertainty` is not `non-answer`. 
* There is `belief_change` if the prior-posterior difference exceeds 0.05, or if the confidence rating changes.
* Assign each `submission_id` a `task_sensitivity` score: 1 - `deviant`/`is_answer`.
* Pandas query string for exclusion: `'attention_score == 1 & reasoning_score > 0.5 & task_sensitivity > 0.75'`.

```{python}
#| code-fold: true
import numpy as np
import pandas as pd
from pathlib import Path

mypath = '/home/omarsagha/code/relevance-of-answers/qualitative-analysis/qualitative-analysis.html'

relevance_dir = Path(mypath).resolve().parent.parent
# INPUT
data_path = relevance_dir / 'qualitative-analysis' / 'data' / 'by_item_with_stimuli.csv'

d = pd.read_csv(data_path)
```

```{python}
#| code-fold: true
def display_stimulus(s):
    '''
    Take a row s in the dataframe and 
    return a string with the context/answer conditions
    and the stimulus text.
    '''
    stimulus_text = f'{s.Context}\n{s.YourQuestionIntro}\n{s.YourQuestion}\n{s.AnswerIntro}\n{s.Answer}\n'
    out = f'AnswerCertainty: `{s.AnswerCertainty}` AnswerPolarity: `{s.AnswerPolarity}` ContextType: `{s.ContextType}`\n\n'
    out += stimulus_text
    return out

def format_float_list(float_list): 
    return '[' + ', '.join(
    # Round all floats to 2 sig digs
        f'{num:.2f}'
        for num in float_list
) + ']'

def format_responses(row, colname_list): 
    '''
    Get the string representation for 
    the observations for each measure.
    '''
    return '\n' + '\n\n'.join(
    f'{col}: {format_float_list(row.loc[col])}'
    for col in colname_list
) + '\n\n'

def compare_cols(df, query, cols):
    '''
    Take a query and a list of columns (measures)
    and return a list of formatted examples
    with useful information.
    '''
    tmp = (d
    .query(query)
    .apply(
        lambda s: display_stimulus(s) + format_responses(s, cols), 
        axis=1))
    return '\n\n'.join(dict(tmp).values())
d = d.assign(
    stimulus=lambda s: display_stimulus(s),
    axis=1)
d['stimulus']
```

```{python}
fig = px.scatter(d,
    x='pri_q50',
    y='pos_q50',
    color='rel_q50',
    hover_data=['stimulus'],
)
# fig = go.Scatter(
#     ???,
#     custom_data=d.stimulus,
#     hovertemplate='{custom_data}',
#     )
# fig.update_traces(mode="markers+lines")

fig.show()
```

## Extreme examples
### BFUtility vs. Relevance
#### Middle quartiles non-overlapping
Examples where BFU is much higher than relevance.
```{python}
#| output: asis
print(compare_cols(d, 
    query= 'rel_q75 <= bfu_q25',
    cols=['rel', 'bfu'],
))
```

Examples where relevance is much higher than BFU.
```{python}
#| output: asis
print(compare_cols(d, 
    query= 'bfu_q75 <= rel_q25',
    cols=['rel', 'bfu'],
))
```

### KLUtility vs. Relevance
#### Middle quartiles non-overlapping
Examples where KLU is much higher than relevance.
```{python}
#| output: asis
print(compare_cols(d, 
    query= 'rel_q75 <= klu_q25',
    cols=['rel', 'klu'],
))
```

Examples where relevance is much higher than KLU.
```{python}
#| output: asis
print(compare_cols(d, 
    query= 'klu_q75 <= rel_q25',
    cols=['rel', 'klu'],
))
```


