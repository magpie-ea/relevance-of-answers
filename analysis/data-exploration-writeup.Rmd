---
title: "Testing quantitative notions of relevance"
author: "Michael Franke"
date: '2022-06-11'
output: 
  html_document:
    toc: true
    toc_depth: 2
    highlight: zenburn
---


```{r setup, include=FALSE}

knitr::opts_chunk$set(echo = TRUE, cache = T, message = FALSE, warning = FALSE, error = FALSE, fig.width = 9, fig.align = "center")

```

```{r libraries, message = FALSE, warning = FALSE, include = FALSE}
library(tidyverse)
library(tidyjson)
# library(GGally)
library(cowplot)
library(BayesFactor)
library(brms)
library(aida)   # custom helpers: https://github.com/michael-franke/aida-package
library(faintr) # custom helpers: https://michael-franke.github.io/faintr/index.html

source('xx-faintr_functions.R')
source('xx-faintr_helpers.R')

# these options help Stan run faster
options(mc.cores = parallel::detectCores())

# use the aida-theme for plotting
theme_set(theme_aida())

# global color scheme / non-optimized
project_colors = c("#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7", "#000000")

# setting theme colors globally
scale_colour_discrete <- function(...) {
  scale_colour_manual(..., values = project_colors)
}
scale_fill_discrete <- function(...) {
  scale_fill_manual(..., values = project_colors)
} 

rerun_models <- FALSE 
```

# Read, inspect & massage the data


The cleaned and pre-processed data comes in JSON format, and contains nested lists, which we here spread out fully into a regular tibble. 

```{r}
# read cleaned data from JSON-lines file and cast to tibble
d <- read_json("../results/round_1.0/results_processed.jsonl") %>% 
  spread_all() %>% 
  select(-..JSON, -document.id) %>% as_tibble() %>% 
  # set "non-answers" to AnswerPolarity "positive"
  mutate(AnswerPolarity = ifelse(AnswerCertainty == "non_answer", "positive", AnswerPolarity))

# add information from (nested) JSON entries on beta-parameters for prior and posterior
prior_beta <- read_json("../results/round_1.0/results_processed.jsonl") %>% 
  enter_object(prior_beta) %>% gather_array() %>% 
  mutate(parameter = ifelse(array.index == 1, "prior_beta_a", "prior_beta_b"),
         value = as.numeric(..JSON)) %>% 
  pivot_wider(id_cols = document.id, names_from = parameter, values_from = value)
posterior_beta <- read_json("../results/round_1.0/results_processed.jsonl") %>% 
  enter_object(posterior_beta) %>% gather_array() %>% 
  mutate(parameter = ifelse(array.index == 1, "posterior_beta_a", "posterior_beta_b"),
         value = as.numeric(..JSON)) %>% 
  pivot_wider(id_cols = document.id, names_from = parameter, values_from = value)

# bind tibbles together
d <- cbind(
  d,
  prior_beta %>% select(-document.id), 
  posterior_beta %>% select(-document.id)
)
```

Here's a glimpse at the data:

```{r}
glimpse(d)
```

Here's a list of what each column represents:

```{r, echo = F}
list(
  "submission_id" = "ID for each participant",
  "group" = "whether the participant saw trigger word 'helpful' or 'relevant'",
  "StimID" = "ID for each stimulus (vignette)",
  "AnswerCertainty" = "whether shown answer in trial was 'exhaustive', 'high_certainty', 'low_certainty' or 'non_answer'",
  "AnswerPolarity" = "whether shown answer in trial was 'positive' (suggesting yes) or 'negative' (suggestion no)",
  "ContextType" = "whether the vignette was realized as making a 'no' answer more likely ('negative'), or a 'yes' answer ('positive') or neither ('neutral')",
  "posterior_confidence" = "slider rating indicating confidence in the corresponding posterior rating",
  "prior_confidence" = "slider rating indicating confidence in the corresponding prior rating",
  "posterior_SliderResponse" = "slider rating indicating posterior belief (= after the answer)",
  "prior_SliderResponse" = "slider rating indicating prior belief (= before the answer)",
  "relevance_sliderResponse" = "slider rating indicating how 'relevant' or 'helpful' the answer was (trigger word depends on 'group' variable)",
  "prior_concentration" = "inferred concetration parameter of a beta distribution representing the participants prior and prior confidence ratings",
  "posterior_concentration" = "inferred concetration parameter of a beta distribution representing the participants posterior and posterior confidence ratings",
  "kl" = "KL divergence from posterior (true distribution) to prior (approximate distribution)",
  "kl_util" = "scaled KL divergence as 1 - (10^{-kl})",
  "entropy_reduction" = "abs(Entropy(prior) - Entropy(posterior))",
  "bayes_factor" = "abs(log(BF)) where BF is the Bayes factor, computed as posterior-odds / prior-odds (based on ratings given for prior and posterior)",
  "exp_bayes_factor" = "scaled Bayes factor, computed as 1 - (10^{-bayes_factor})",
  "posterior_distance" = "distance of posterior from uniform distribution, calculated as 2 * abs(0.5 - prior)",
  "prior_posterior_distance" = "distance between prior and posterior, calculated as abs(prior - posterior)",
  "kl_beta" = "KL divergence between the higher-order prior & posterior distributions, which are computed from the prior/posterior slider ratings and the corresponding confidence ratings",
  "kl_util_beta" = "scaled kl_beta computed as 2^{-kl_beta}",
  "entropy_reduction_beta" = "Entropy(higher-order prior) - Entropy(higher-order posterior)",
  "prior_beta_a" = "alpha parameter of the (inferred) beta distribution describing the participants prior beliefs, based on ratings for 'prior' and 'prior_confidence'",
  "prior_beta_b" = "beta parameter of the (inferred) beta distribution describing the participants prior beliefs, based on ratings for 'prior' and 'prior_confidence'",
  "posterior_beta_a" = "alpha parameter of the (inferred) beta distribution describing the participants posterior beliefs, based on ratings for 'posterior' and 'posterior_confidence'",
  "posterior_beta_b" = "beta parameter of the (inferred) beta distribution describing the participants posterior beliefs, based on ratings for 'posterior' and 'posterior_confidence'"
)
```

For convenience, a bit of renaming and releveling:

```{r }
d <- d %>% 
  mutate(
    "trigger word" = factor(group, levels = c("relevant", "helpful")),
    "relevance rating" = relevance_sliderResponse,
    ContextType = factor(ContextType, levels = c("negative", "neutral", "positive")),
    AnswerPolarity = factor(AnswerPolarity, levels = c("positive", "negative")),
    AnswerPolarity = factor(AnswerPolarity, levels = c("positive", "negative")),
    AnswerCertainty = factor(AnswerCertainty, 
                             levels = c("non_answer", "low_certainty", "high_certainty", "exhaustive"))
  ) 
```

# Brief data inspection

```{r}
# 71 subjects after cleaning
d %>% pull(submission_id) %>% unique() %>% length()

# each gave 9 or 10 judgements
table(d$submission_id)

# participants either got questions after "relevance" or "helpfulness"
d %>% group_by(submission_id, group) %>% 
  summarize(n = n())

# items were classified in terms of ContextType, AnswerCertainty and AnswerPolarity
d %>% group_by(StimID, ContextType, AnswerCertainty, AnswerPolarity) %>% 
  count()
```

# Exploring the effect of the experimental factors

The experiment had the following factors:

- `group` or `trigger word`: whether a participant rated the 'helpfulness' or the 'relevance' of the answer (between-subjects variable)
- `ContextType`: whether the context made a 'no' or a 'yes' answer more likely /a priori/ or whether it was neutral (within-subjects)
- `AnswerCertainty`: how much information the answer provides towards a fully resolving answer (within-subjects)
- `AnswerPolarity`: whether the answer suggests or implies a 'no' or a 'yes' answer (within-subjects)
  - 'non-answers' are treated as 'positive' for technical purposes, but this does not influence relevant analyses

In the following, we first check whether these experimental manipulations worked as intended.
Then we derive some hypotheses about the effect of these manipulations from different notions of relevance.

## Sanity-checking whether the manipulations were as intended

### Effects of `ContextType` on prior and prior confidence

To check whether the `ContextType` manipulation worked, we compare how participants rated the prior probability of a 'yes' answer under each level of the `ContextType` variable. 
Concretely, we expect this order of prior ratings for the levels of `ContextType`: negative < neutral < positive.
Although we have no specific hypotheses or sanity-checking questions regarding the confidence ratings, let's also scrutinize the confidence ratings that participants gave with their prior ratings.

#### Prior ratings as a function of `ContextType`

Here is a first plot addressing the question after an effect of `ContextType` on participants prior ratings.

```{r}
d %>% ggplot(aes(x = prior_sliderResponse, color = ContextType, fill = ContextType)) +
  geom_density(alpha = 0.3) + 
  xlab("prior rating") +
  ylab("")
```

We dive deeper by fitting a regression model, predicting prior ratings in terms of the `ContextType`.
Since participants have not seen the answer when they rate the prior probability of a 'yes' answer, `ContextType` is the only fixed effect we should include here.
The model also includes the maximal RE structure.

```{r, results = 'hide'}
if (rerun_models) {
  fit_contextType_SC <- brm(
    prior_sliderResponse ~ ContextType + 
      (1 + ContextType | StimID) + 
      (1 + ContextType | submission_id),
    data = d
  )
  saveRDS(fit_contextType_SC, "cachedModels/fit_contextType_SC.Rds")
} else{
  fit_contextType_SC <- readRDS("cachedModels/fit_contextType_SC.Rds")
}
```

Our assumption is that prior ratings are higher in contexts classified as 'neutral' than in 'negative' contexts, and yet higher in 'positive' contexts.
We use the `faintr` package to extract information on these directed comparisons.

```{r}
ContextType_SC_negVSneu <- 
  compare_groups(
    fit_contextType_SC, 
    lower = ContextType == "negative",
    higher = ContextType == "neutral" 
  )
ContextType_SC_neuVSpos <- 
  compare_groups(
    fit_contextType_SC, 
    lower = ContextType == "neutral",
    higher = ContextType == "positive"
  )
```

#### Prior confidence as a function of `ContextType`

Here is a visualization of the effect of `ContextType` on participants' confidence in their prior ratings.

```{r}
d %>% ggplot(aes(x = prior_confidence, color = ContextType, fill = ContextType)) +
  geom_density(alpha = 0.3) + 
  xlab("prior confidence") +
  ylab("")

```

To scrutinize the effect of `ContextType` on participants expressed confidence in their prior ratings, we use a similar regression model and gather information about the contrast which -upon visual inspection- seem most likely to be meaningful.

```{r, results = 'hide'}
if (rerun_models) {
  fit_contextType_SC_conf <- brm(
    prior_confidence ~ ContextType + 
      (1 + ContextType | StimID) + 
      (1 + ContextType | submission_id),
    data = d
  )
  saveRDS(fit_contextType_SC_conf, "cachedModels/fit_contextType_SC_conf.Rds")
} else{
  fit_contextType_SC_conf <- readRDS("cachedModels/fit_contextType_SC_conf.Rds")
}
```


```{r, results = 'hide'}
ContextType_SC_neuVSneg_conf <- 
  compare_groups(
    fit_contextType_SC_conf, 
    lower = ContextType == "neutral",
    higher = ContextType == "negative" 
  )
ContextType_SC_negVSpos_conf <- 
  compare_groups(
    fit_contextType_SC_conf, 
    lower = ContextType == "negative",
    higher = ContextType == "positive"
  )
```

#### Results

The results of these comparisons are summarized here:

```{r, echo = F}
results_ContextType_SanityCheck <- 
  tribble(
  ~comparison, ~measure, ~posterior, ~"95% HDI (low)", ~"95% HDI (high)",
  "negative < neutral" , "prior", ContextType_SC_negVSneu$post_prob, ContextType_SC_negVSneu$l_ci, ContextType_SC_negVSneu$u_ci,  
  "neutral < positive" , "prior", ContextType_SC_neuVSpos$post_prob, ContextType_SC_neuVSpos$l_ci, ContextType_SC_neuVSpos$u_ci,  
  "neutral < negative" , "prior-confidence", ContextType_SC_neuVSneg_conf$post_prob, ContextType_SC_neuVSneg_conf$l_ci, ContextType_SC_neuVSneg_conf$u_ci,  
  "negative < positive" , "prior-confidence", ContextType_SC_negVSpos_conf$post_prob, ContextType_SC_negVSpos_conf$l_ci, ContextType_SC_negVSpos_conf$u_ci,  
  )

knitr::kable(results_ContextType_SanityCheck)
```

The `ContextType` manipulation seems to have worked as expected for the prior ratings: lower in 'negative' than in 'neutral' than in 'positive'.
There are also differences in the confidence ratings, with lowest confidence in the 'neutral' contexts (which makes sense), but no indication of a difference between 'negative' having less confidence than 'positive'.


#### Further exploration

Finally, this is an interesting plot, showing the relation between prior ratings and prior confidence, also in dependence on `ContextType`.
We see that, unsurprisingly, the value 0.5 for prior rattings has an exceptional status, attracting a lot of data points from the whole range of prior confidence.
Ignoring this value, there is a seemingly U-shaped curve, suggesting a trend to be more confident in ratings further away from 0.5.
[We could speak of **Laplacian confidence**: total confidence that we have absolutely no idea, hence prior=0.5. That level of confidence is not attainable, it seems, for 0.49, unless we'd have very specific information.]

```{r}
d %>% ggplot(aes(x = prior_sliderResponse, y = prior_confidence, color = ContextType)) +
  geom_jitter(height = 0.3, width =0, alpha = 0.7) +
  # geom_point(alpha = 0.7) +
  xlab("prior rating") +
  ylab("prior confidence") 
```

We also check this for the posterior:

```{r}
d %>% ggplot(aes(x = posterior_sliderResponse, y = posterior_confidence, color = ContextType)) +
  geom_jitter(height = 0.3, width =0, alpha = 0.7) +
  # geom_point(alpha = 0.7) +
  xlab("posterior rating") +
  ylab("posterior confidence") 
```
How does the change in confidence relate to the change in extremity? We use "extremity" to refer to the distance of the probability judgment from 0.5.

```{r}
d$extremity_delta <- abs(0.5 - d$posterior_sliderResponse) - abs(0.5 - d$prior_sliderResponse)
d$confidence_delta <- d$posterior_confidence - d$prior_confidence
d %>% ggplot(aes(x = extremity_delta, y = confidence_delta, color = ContextType)) +
  geom_jitter(height = 0.3, width =0, alpha = 0.7) +
  # geom_point(alpha = 0.7) +
  xlab("change in extremity (distance from 0.5) from prior to posterior") +
  ylab("change in posterior confidence") 
```
The general trend is as expected: If the posterior probability is more extreme than the prior, the confidence judgment also tends to be high. Upon visual inspection, the relationship seems fairly linear. However, in the minority of cases where extremity decreases substantially (these are cases where one had an overly confident prior), the confidence rating does not decrease as much as expected given a linear trend. One interpretation of this is that there is a slight bias towards increasing confidence from prior to posterior, independent of the change in extremity of the probability. That is, more information tends to make people more confident in their probability estimates, even if it makes them less confident in the actual answer.


```{r}
# TODO: [possibly] check if neutral ContextType were rated roughly around 0.5 (I guess visualization above suffices?)
```

### Effects of `AnswerPolarity` and `AnswerCertainty` on `beliefChange`

We can define `beliefChange` as the difference between posterior and prior *in the direction expected from the answer's polarity* (posterior belief in 'yes' answer increases for a 'positive' answer when compared with the prior rating, but it decreases for 'negative' answers).
(Careful: we ignore non-answers (which are categorized as "positive" for technical convenience only).)
If our manipulation worked, we expect the following for both 'positive' and 'negative' polarity:

1. `beliefChange` is > 0 
2. `beliefChange` is lower for 'low certainty' than for 'high certainty' than for 'exhaustive'

Here is a plot visually addressing these issues:

```{r}
d %>% filter(AnswerCertainty != "non_answer") %>% 
  mutate(beliefChange = posterior_sliderResponse - prior_sliderResponse,
         beliefChange = ifelse(AnswerPolarity == "positive", beliefChange, - beliefChange)) %>% 
  ggplot(aes(x = beliefChange, color = AnswerCertainty, fill = AnswerCertainty)) +
  geom_density(alpha = 0.3) +
  facet_grid(AnswerPolarity ~ AnswerCertainty) +
  xlab("belief change (in expected direction)") +
  ylab("") + theme_aida()
```

#### `beliefChange` is positive

To adress the first issue, whether `beliefChange` is positive for both types of polartiy, we first regress `beliefChange` against the full list of potentially relevant factors, including plausible RE structures.
Notice that at the time of answer the questions related to the posterior, participants have not yet seen the question after relevance or helpfulness, so that factor `group` (equivalently: `trigger word`) should be ommitted.

```{r, results = 'hide'}
if (rerun_models) {
  fit_answer_SC <- brm(
    formula = beliefChange ~ ContextType * AnswerCertainty * AnswerPolarity +
      (1 + ContextType + AnswerCertainty + AnswerPolarity | StimID) +
      (1 + ContextType + AnswerCertainty + AnswerPolarity | submission_id),
    data = d %>% filter(AnswerCertainty != "non_answer") %>% 
      mutate(beliefChange = posterior_sliderResponse - prior_sliderResponse,
             beliefChange = ifelse(AnswerPolarity == "positive", beliefChange, - beliefChange))
  )
  saveRDS(fit_answer_SC, "cachedModels/fit_answer_SC.Rds")
} else{
  fit_answer_SC <- readRDS("cachedModels/fit_answer_SC.Rds")
}
```


We check if inferred cell means are credibly bigger than zero, for all six relevant design cells (facets in the plot above).

```{r}
# 1. Check if belief change in each cell is bigger than zero
cellDraws_answers <- tibble(
  extract_cell_draws(fit_answer_SC, AnswerCertainty == "low_certainty" & AnswerPolarity == "positive", "low_pos"),
  extract_cell_draws(fit_answer_SC, AnswerCertainty == "high_certainty" & AnswerPolarity == "positive", "high_pos"),
  extract_cell_draws(fit_answer_SC, AnswerCertainty == "exhaustive"    & AnswerPolarity == "positive", "exh_pos"),
  extract_cell_draws(fit_answer_SC, AnswerCertainty == "low_certainty" & AnswerPolarity == "negative", "low_neg"),
  extract_cell_draws(fit_answer_SC, AnswerCertainty == "high_certainty" & AnswerPolarity == "negative", "high_neg"),
  extract_cell_draws(fit_answer_SC, AnswerCertainty == "exhaustive"    & AnswerPolarity == "negative", "exh_neg")
) 

# all posterior 95% HDIs are wayquire above 0 
cellDraws_answers %>% as.matrix() %>% 
  apply(., 2, aida::summarize_sample_vector)

# posterior probability of mean bigger 0 for each cell is almost 1 everywhere
as.matrix(cellDraws_answers) %>% 
  apply(., 2, function(x) {mean(x>0)})
```

These results suggest that there is little reason to doubt that the belief changes induces by the answers -as per the experimentally intended manipulation- went in the right direction in all cases.

```{r}
# TODO make a plot of the posterior cell draws, include 95% HDIs there (more perspicuous than the numbers)
```

#### `beliefChange` increases with more informative answers

Finally, we investigate whether `beliefChange` increases with more informative answers, using the same regression model as before.
```{r}

AnswerPolarity_main <- compare_groups(
  fit_answer_SC,
  lower = AnswerPolarity == "positive",
  higher  = AnswerPolarity == "negative"
)

AnswerCertainty_lowVShigh <- compare_groups(
  fit_answer_SC,
  lower   = AnswerCertainty == "low_certainty",
  higher  = AnswerCertainty == "high_certainty"
)

AnswerCertainty_highVSexh <- compare_groups(
  fit_answer_SC,
  lower   = AnswerCertainty == "high_certainty",
  higher  = AnswerCertainty == "exhaustive"
)

```

```{r, echo = F}
results_answer_SC <- tribble(
  ~comparison, ~measure, ~posterior, ~"95%HDI (low)", ~"95%HDI (high)",
  "pos vs neg polarity" , "belief change", AnswerPolarity_main$post_prob, AnswerPolarity_main$l_ci, AnswerPolarity_main$u_ci,  
  "low vs high certainty" , "belief change", AnswerCertainty_lowVShigh$post_prob, AnswerCertainty_lowVShigh$l_ci, AnswerCertainty_lowVShigh$u_ci,  
  "high certain vs exh" , "belief change", AnswerCertainty_highVSexh$post_prob, AnswerCertainty_highVSexh$l_ci, AnswerCertainty_highVSexh$u_ci  
)

knitr::kable(results_answer_SC)
```

We see no indication of a main effect of polarity, but find support for the claim that our manipulation of AnswerCertainty induced gradually larger belief changes.
I sum, it seems that the stimuli were adequately created to implement the intended manipulation in the variables `AnswerCertainty` and `AnswerPolarity`.


### Exploring the "no belief change cases"

A "no belief change" case is a trial in which the prior judgement equals the posterior judgement and also the ratings of confidence in each are exactly the same.
These are expected to happen only for 'non-answers' or for cases where the prior was already extreme (in the direction of the polarity of the answer).
It should also be checked if individual participants are particularly prone to indicating no belief change at all.

```{r, eval = F}
d %>% mutate(
  noBeliefChange = case_when(
    prior_sliderResponse == posterior_sliderResponse &
      prior_confidence == posterior_confidence ~ TRUE,
    TRUE ~ FALSE
  )
) %>%
  select(submission_id, AnswerCertainty, prior_sliderResponse, posterior_sliderResponse ,
      prior_confidence, posterior_confidence, noBeliefChange) %>% 
  View()
```

```{r}
# TODO check if this looks okay; any individuals particularly prone to no-belief-changes?
```


## Predicting relevance in terms of the experimental factors

To see how participants' relevance ratings depend on the experimental manipulations, we run a linear regression model predicting the former in terms of the latter:

```{r, results = "hide"}
# I'm ommitting interactions in the REs because of the unbalanced (factorial) design
# TODO rethink this
formula = relevance_sliderResponse ~ group * ContextType * AnswerCertainty * AnswerPolarity + 
  (1 + group + ContextType + AnswerCertainty + AnswerPolarity | StimID) + 
  (1 + ContextType + AnswerCertainty + AnswerPolarity | submission_id)

# priors must be specified b/c with improper priors posterior is improper as well
prior = prior(student_t(1, 0, 1), class = "b")

if (rerun_models) {
  fit <- brm(
    formula = formula,  
    iter = 4000,
    prior = prior,
    data = d
  )
  saveRDS(fit, "cachedModels/fit.Rds")
} else{
  fit <- readRDS("cachedModels/fit.Rds")
}
```

### Can we gloss over the different trigger words?

To simplify analyses, it would be helpful to know whether we can gloss over the `trigger word` manipulation.
So, does it matter whether participants were asked to rate *relevance* or *helpfulness*?

To start with, let's just look at whether there is a main effect, which there is not (possibly also partially explained away by by-subject random slopes):

```{r, results = "hide"}
# main effect of "group" ?
group_main <- compare_groups(
  fit,
  higher = group == "relevant",
  lower  = group == "helpful"
)
group_main
```

Alternatively, we may compare models with and without the `group` factors.
Starting with simple Bayes factors (no REs):

```{r, results = 'hide'}
BFComp_group_full <- lmBF(relevance_sliderResponse ~ group * ContextType * AnswerCertainty * AnswerPolarity,
  data = as.data.frame(d))
BFComp_group_noGroup <- lmBF(relevance_sliderResponse ~ ContextType * AnswerCertainty * AnswerPolarity,
  data = as.data.frame(d))
```

We find that the model without `group` has a very high Bayes factor in its favor:

```{r}
BFComp_group_noGroup / BFComp_group_full
```

This also shows in this plot:

```{r}
model_names <- c("full", "lesioned")

tibble(
  full      = BFComp_group_full %>% as.vector(), 
  lesioned  = BFComp_group_noGroup %>% as.vector(),
) %>% pivot_longer(everything(), names_to = "model", values_to = "relative_BF") %>% 
  mutate(
    # model = fct_reorder(model, relative_BF)
    model = factor(model, rev(model_names))
  ) %>% 
  ggplot(aes(x = model, y = relative_BF)) +
  scale_y_log10() +
  geom_col() +
  coord_flip() +
  xlab("") +
  ylab("log BF (relative to intercept only)")

```



Finally, let's also compare with LOO cross-validation. 
We first run the lesioned model:

```{r, results = 'hide'}
if (rerun_models) {
  fit_noGroup <- brm(
    formula = relevance_sliderResponse ~ ContextType * AnswerCertainty * AnswerPolarity + 
      (1 + ContextType + AnswerCertainty + AnswerPolarity | StimID) + 
      (1 + ContextType + AnswerCertainty + AnswerPolarity | submission_id),  
    iter = 4000,
    prior = prior,
    data = d
  )
  saveRDS(fit_noGroup, "cachedModels/fit_noGroup.Rds")
} else{
  fit_noGroup <- readRDS("cachedModels/fit_noGroup.Rds")
}
```

And then comparing via LOO:

```{r}
# add LOO information
fit <- add_criterion(fit, "loo", model_name = "full")
fit_noGroup <- add_criterion(fit_noGroup, "loo", model_name = "w/o group")

# compare models by LOO
loo_compare(
  fit,   
  fit_noGroup
)  
```

It appears that, when comparing trained models with REs, the lesioned model is slightly worse numerically, but not with a margin large enough to strongly prefer the full model.
We take all of this as an indication that we may proceed (temporarily, in visualizations) with partial neglect of the `trigger word` distinction. 

### Effect of `AnswerPolarity`, `AnswerCertainty` and `ContextType` on relevance ratings

To investigate further which experimental factors influence the ratings of relevance of an answer, start by a visualization:

```{r, echo = F, eval = F}
# p_relevant <- d %>% 
#   filter(group == "relevant") %>% 
#   ggplot(aes(x = `relevance rating`, color = AnswerPolarity, fill = AnswerPolarity)) +
#   facet_grid(AnswerCertainty ~ ContextType , scales = "free") +
#   geom_density(alpha = 0.3) +
#   ggtitle("trigger word: 'relevant'")
# 
# p_helpful <- d %>% 
#   filter(group == "helpful") %>% 
#   ggplot(aes(x = `relevance rating`, color = AnswerPolarity, fill = AnswerPolarity)) +
#   facet_grid(AnswerCertainty ~ ContextType , scales = "free") +
#   geom_density(alpha = 0.3) +
#   ggtitle("trigger word: 'helpful'")
# 
# plot_grid(p_relevant, p_helpful, nrow=1)
```

```{r}
d %>% 
  ggplot(aes(x = `relevance rating`, color = AnswerPolarity, fill = AnswerPolarity)) +
  facet_grid(AnswerCertainty ~ ContextType , scales = "free") +
  geom_density(alpha = 0.3) + theme_aida()
```

Let's now look at a bunch of contrasts (based on the previously fitted full model).

```{r, echo = F}
## expected ordering relation?
## non-answers vs low-certainty => poster = 1
nonAns_VS_low  <- compare_groups(
  fit,
  lower  = AnswerCertainty == "non_answer",
  higher = AnswerCertainty == "low_certainty"
)
## low-certainty vs high-certainty => poster = 0.9922
low_VS_high <- compare_groups(
  fit,
  lower  = AnswerCertainty == "low_certainty",
  higher = AnswerCertainty == "high_certainty"
)
## high-certainty vs exhaustive => poster = 1
high_VS_exh <- compare_groups(
  fit,
  lower  = AnswerCertainty == "high_certainty",
  higher = AnswerCertainty == "exhaustive"
)


# TODO check if that also holds for comparisons within each group

## effects of AnswerPolarity?
AnswerPolarity_main <- compare_groups(
  fit,
  lower  = AnswerPolarity == "positive" & AnswerCertainty != "non_answer",
  higher = AnswerPolarity == "negative" & AnswerCertainty != "non_answer"
)

AnswerPolarity_lowCertain <- compare_groups(
  fit,
  lower  = AnswerPolarity == "positive" & AnswerCertainty == "low_certainty",
  higher = AnswerPolarity == "negative" & AnswerCertainty == "low_certainty"
)

AnswerPolarity_highCertain <-compare_groups(
  fit,
  lower  = AnswerPolarity == "positive" & AnswerCertainty == "high_certainty",
  higher = AnswerPolarity == "negative" & AnswerCertainty == "high_certainty"
)

AnswerPolarity_exhaustive <-compare_groups(
  fit,
  lower  = AnswerPolarity == "positive" & AnswerCertainty == "exhaustive",
  higher = AnswerPolarity == "negative" & AnswerCertainty == "exhaustive"
)

ContextType_neutral <- 
  compare_groups(fit, higher = ContextType == "neutral", lower = ContextType != "neutral")

cellComparisons <- tribble(
  ~comparison, ~posterior, ~"95%HDI (low)", ~"95%HDI (high)",
  # "group: helpful < relevance" , group_main$post_prob, group_main$l_ci, group_main$u_ci,  
  "non-answer < low certainty" , nonAns_VS_low$post_prob, nonAns_VS_low$l_ci, nonAns_VS_low$u_ci,  
  "low certain < high certain" , low_VS_high$post_prob, low_VS_high$l_ci, low_VS_high$u_ci,  
  "high certain < exhaustive" , high_VS_exh$post_prob, high_VS_exh$l_ci, high_VS_exh$u_ci,  
  "answer: pos < neg", AnswerPolarity_main$post_prob, AnswerPolarity_main$l_ci, AnswerPolarity_main$u_ci,
  # "Polarity (low certain)", AnswerPolarity_lowCertain$post_prob, AnswerPolarity_lowCertain$l_ci, AnswerPolarity_lowCertain$u_ci,
  # "Polarity (high certain)", AnswerPolarity_highCertain$post_prob, AnswerPolarity_highCertain$l_ci, AnswerPolarity_highCertain$u_ci,
  # "Polarity (exhaustive)", AnswerPolarity_exhaustive$post_prob, AnswerPolarity_exhaustive$l_ci, AnswerPolarity_exhaustive$u_ci,
  "context: neutral > pos/neg", ContextType_neutral$post_prob, ContextType_neutral$l_ci, ContextType_neutral$u_ci
)

knitr::kable(cellComparisons)
```

The table shows results indicating that there are (non-surprising) effects of `AnswerType` with non-answers rated as least relevant, followed by low-certainty, then high-certainty answers, and finall exhaustive answers.
There is no (strong) indication for a main effect of `AnswerPolarity` or `ContextType`.
The lack of an effect of `ContextType` might be interpreted as *prima facie* evidence in favor of quantitative notions or relevance that do not take the prior into account (at least not very prominently). 

Here is a plot of the relevant posterior draws visually supporting why we compared the three factor levels of `ContextType` in the way we did:

```{r}
draws_ContextType <- 
  tibble(
    extract_cell_draws(fit, ContextType == "positive", colname = "positive"),
    extract_cell_draws(fit, ContextType == "negative", colname = "negative"),
    extract_cell_draws(fit, ContextType == "neutral", colname = "neutral")
  ) %>% pivot_longer(cols = everything())

draws_ContextType %>% 
  ggplot(aes(x = value, color = name, fill = name)) +
  geom_density(alpha = 0.3)
```

# Exploring QuaRels


